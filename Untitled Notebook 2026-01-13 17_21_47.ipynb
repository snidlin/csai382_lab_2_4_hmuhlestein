{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d694f5db-a664-4a91-8f4b-9b91ee86e595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(25):\n",
    "        if (cur / \"logs\").exists() and (cur / \"data\").exists():\n",
    "            return cur\n",
    "        if (cur / \"RUN.md\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"Repo root not found. Make sure the notebook is created inside the repo.\")\n",
    "\n",
    "REPO_ROOT = find_repo_root(Path.cwd())\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "LOGS_DIR = REPO_ROOT / \"logs\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "run_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "log_file = LOGS_DIR / f\"run_{run_ts}.log\"\n",
    "\n",
    "logger = logging.getLogger(\"lab_2_4\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers.clear()\n",
    "logger.propagate = False\n",
    "\n",
    "fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "ch = logging.StreamHandler(stream=sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(fmt)\n",
    "\n",
    "fh = logging.FileHandler(log_file, mode=\"w\", encoding=\"utf-8\")\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(fmt)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "logger.info(\"=== START RUN ===\")\n",
    "logger.info(f\"Repo root: {REPO_ROOT}\")\n",
    "logger.info(f\"Data dir : {DATA_DIR}\")\n",
    "logger.info(f\"Logs dir : {LOGS_DIR}\")\n",
    "logger.info(f\"Log file : {log_file}\")\n",
    "logger.info(f\"Python   : {sys.version.split()[0]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, random\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "logger.info(\"Reproducibility seeds set: PYTHONHASHSEED=0, random=0, numpy=0\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import json, glob, hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "csvs = sorted([Path(p) for p in glob.glob(str(DATA_DIR / \"*.csv\"))])\n",
    "if len(csvs) < 2:\n",
    "    raise FileNotFoundError(f\"Expected 2 CSVs in {DATA_DIR}, found {len(csvs)}. Add them and rerun.\")\n",
    "\n",
    "hashes = {}\n",
    "for p in csvs:\n",
    "    digest = sha256_file(p)\n",
    "    hashes[p.name] = {\"sha256\": digest, \"bytes\": p.stat().st_size}\n",
    "    logger.info(f\"Input hash | {p.name} | sha256={digest} | bytes={p.stat().st_size}\")\n",
    "\n",
    "hash_path = REPO_ROOT / \"data_hashes.json\"\n",
    "hash_path.write_text(json.dumps(hashes, indent=2), encoding=\"utf-8\")\n",
    "logger.info(f\"Wrote hashes to: {hash_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If you have exact filenames, set them here. Otherwise it uses the first two CSVs found.\n",
    "menu_path = DATA_DIR / \"menu_items.csv\"\n",
    "orders_path = DATA_DIR / \"order_details.csv\"\n",
    "\n",
    "if not menu_path.exists() or not orders_path.exists():\n",
    "    logger.info(\"Canonical filenames not found; using first two CSVs found in data/.\")\n",
    "    menu_path, orders_path = csvs[:2]\n",
    "\n",
    "logger.info(f\"Loading menu file  : {menu_path.name}\")\n",
    "logger.info(f\"Loading orders file: {orders_path.name}\")\n",
    "\n",
    "menu_items = pd.read_csv(menu_path)\n",
    "order_details = pd.read_csv(orders_path)\n",
    "\n",
    "logger.info(f\"Loaded menu_items    shape={menu_items.shape}\")\n",
    "logger.info(f\"Loaded order_details shape={order_details.shape}\")\n",
    "logger.info(f\"menu_items columns    = {list(menu_items.columns)}\")\n",
    "logger.info(f\"order_details columns = {list(order_details.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def trim_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    obj_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    for c in obj_cols:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "menu_items = trim_strings(menu_items)\n",
    "order_details = trim_strings(order_details)\n",
    "\n",
    "# Coerce types\n",
    "if \"price\" in menu_items.columns:\n",
    "    menu_items[\"price\"] = pd.to_numeric(menu_items[\"price\"], errors=\"coerce\")\n",
    "\n",
    "if \"menu_item_id\" in menu_items.columns:\n",
    "    menu_items[\"menu_item_id\"] = pd.to_numeric(menu_items[\"menu_item_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "for c in [\"order_id\", \"item_id\"]:\n",
    "    if c in order_details.columns:\n",
    "        order_details[c] = pd.to_numeric(order_details[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "if \"quantity\" in order_details.columns:\n",
    "    order_details[\"quantity\"] = pd.to_numeric(order_details[\"quantity\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "else:\n",
    "    order_details[\"quantity\"] = 1  # fallback if quantity isn't provided\n",
    "\n",
    "# Datetime parsing (your dataset: \"1/1/23 11:38:36 AM\")\n",
    "if \"order_date\" in order_details.columns and \"order_time\" in order_details.columns:\n",
    "    dt_str = (\n",
    "        order_details[\"order_date\"].astype(str).str.strip()\n",
    "        + \" \"\n",
    "        + order_details[\"order_time\"].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    order_details[\"_order_dt\"] = pd.to_datetime(\n",
    "        dt_str,\n",
    "        format=\"%m/%d/%y %I:%M:%S %p\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Parsed datetime from order_date+order_time -> _order_dt \"\n",
    "        f\"(non-null: {order_details['_order_dt'].notna().sum()}, null: {order_details['_order_dt'].isna().sum()})\"\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"order_date/order_time not both present; falling back to date-only parsing.\")\n",
    "    order_details[\"_order_dt\"] = pd.to_datetime(order_details[\"order_date\"], format=\"%m/%d/%y\", errors=\"coerce\")\n",
    "\n",
    "\n",
    "\n",
    "# Join\n",
    "if \"menu_item_id\" not in menu_items.columns or \"item_id\" not in order_details.columns:\n",
    "    raise KeyError(\"Join keys missing. Need menu_items.menu_item_id and order_details.item_id (or adjust code to match your columns).\")\n",
    "\n",
    "tidy = order_details.merge(\n",
    "    menu_items,\n",
    "    left_on=\"item_id\",\n",
    "    right_on=\"menu_item_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_order\", \"_menu\")\n",
    ")\n",
    "\n",
    "# Keep a tidy subset if present\n",
    "cols_wanted = [\"order_id\", \"_order_dt\", \"item_id\", \"menu_item_id\", \"item_name\", \"category\", \"price\", \"quantity\"]\n",
    "tidy = tidy[[c for c in cols_wanted if c in tidy.columns]].copy()\n",
    "\n",
    "# Revenue\n",
    "if \"price\" in tidy.columns:\n",
    "    tidy[\"revenue\"] = tidy[\"price\"].fillna(0) * tidy[\"quantity\"].fillna(0)\n",
    "else:\n",
    "    tidy[\"revenue\"] = 0.0\n",
    "\n",
    "logger.info(f\"Tidy table shape={tidy.shape}\")\n",
    "\n",
    "# Metrics\n",
    "top5_items = (tidy.groupby(\"item_name\")[\"quantity\"].sum().sort_values(ascending=False).head(5).reset_index()\n",
    "              .rename(columns={\"quantity\": \"total_quantity\"})) if \"item_name\" in tidy.columns else pd.DataFrame()\n",
    "\n",
    "revenue_by_category = (tidy.groupby(\"category\")[\"revenue\"].sum().sort_values(ascending=False).reset_index()\n",
    "                       .rename(columns={\"revenue\": \"total_revenue\"})) if \"category\" in tidy.columns else pd.DataFrame()\n",
    "\n",
    "busiest_hour = pd.DataFrame()\n",
    "if tidy[\"_order_dt\"].notna().any():\n",
    "    busiest_hour = (tidy.dropna(subset=[\"_order_dt\"])\n",
    "                    .assign(hour=lambda d: d[\"_order_dt\"].dt.hour)\n",
    "                    .groupby(\"hour\")[\"order_id\"].count()\n",
    "                    .sort_values(ascending=False).head(1).reset_index()\n",
    "                    .rename(columns={\"order_id\": \"order_row_count\"}))\n",
    "\n",
    "logger.info(f\"Top5 items rows         : {len(top5_items)}\")\n",
    "logger.info(f\"Revenue by category rows : {len(revenue_by_category)}\")\n",
    "logger.info(f\"Busiest hour rows        : {len(busiest_hour)}\")\n",
    "\n",
    "print(top5_items.head().to_string(index=False))\n",
    "print(revenue_by_category.head().to_string(index=False))\n",
    "\n",
    "if busiest_hour is not None and len(busiest_hour) > 0:\n",
    "    print(busiest_hour.to_string(index=False))\n",
    "else:\n",
    "    logger.info(\"busiest_hour is empty (no valid _order_dt values parsed).\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"etl_output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "out_path = OUT_DIR / f\"metrics_{out_ts}.csv\"\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "for _, r in top5_items.iterrows():\n",
    "    rows.append({\"metric\": \"top_item_quantity\", \"key\": str(r[\"item_name\"]), \"value\": float(r[\"total_quantity\"])})\n",
    "\n",
    "for _, r in revenue_by_category.iterrows():\n",
    "    rows.append({\"metric\": \"revenue_by_category\", \"key\": str(r[\"category\"]), \"value\": float(r[\"total_revenue\"])})\n",
    "\n",
    "if len(busiest_hour) > 0:\n",
    "    r = busiest_hour.iloc[0]\n",
    "    rows.append({\"metric\": \"busiest_hour\", \"key\": str(int(r[\"hour\"])), \"value\": float(r[\"order_row_count\"])})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "metrics_df[\"metric\"] = metrics_df[\"metric\"].astype(\"string\")\n",
    "metrics_df[\"key\"] = metrics_df[\"key\"].astype(\"string\")\n",
    "metrics_df[\"value\"] = pd.to_numeric(metrics_df[\"value\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"etl_output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out_ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "out_path = OUT_DIR / f\"metrics_{out_ts}.csv\"\n",
    "metrics_df.to_csv(out_path, index=False)\n",
    "\n",
    "logger.info(f\"Saved metrics CSV to: {out_path}\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "\n",
    "required_cols = {\"order_id\", \"item_id\", \"quantity\", \"revenue\"}\n",
    "missing = required_cols - set(tidy.columns)\n",
    "assert not missing, f\"Missing required columns in tidy: {missing}\"\n",
    "\n",
    "assert len(tidy) > 0, \"Tidy table is empty — check inputs/join.\"\n",
    "assert len(metrics_df) > 0, \"Metrics output is empty — check calculations.\"\n",
    "\n",
    "logger.info(\"All asserts passed.\")\n",
    "logger.info(\"=== END RUN ===\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-01-13 17_21_47",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
